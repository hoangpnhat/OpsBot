{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxrYw8WsUy6b"
      },
      "source": [
        "# Property Graph Index\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "In this notebook, we demonstrate some basic usage of the `PropertyGraphIndex` in LlamaIndex.\n",
        "\n",
        "The property graph index here will take unstructured documents, extract a property graph from it, and provide various methods to query that graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9X2TSGlUy6k",
        "outputId": "2812f80d-cfd0-4f2b-c45d-1303af26be60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.46-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl.metadata (678 bytes)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core==0.10.46 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.46-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.22-py3-none-any.whl.metadata (559 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.25-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting PyYAML>=6.0.1 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached SQLAlchemy-2.0.30-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting httpx (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl.metadata (760 bytes)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from llama-index-core==0.10.46->llama-index) (1.6.0)\n",
            "Collecting networkx>=3.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting numpy<2.0.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting openai>=1.1.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pandas (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting pillow>=9.0.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached pillow-10.3.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting requests>=2.31.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tenacity<8.4.0,>=8.2.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached tiktoken-0.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from llama-index-core==0.10.46->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.46->llama-index)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting wrapt (from llama-index-core==0.10.46->llama-index)\n",
            "  Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.46->llama-index)\n",
            "  Downloading pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m194.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting anyio (from httpx->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting certifi (from httpx->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting idna (from httpx->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting sniffio (from httpx->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core==0.10.46->llama-index)\n",
            "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.46->llama-index)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from pandas->llama-index-core==0.10.46->llama-index) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from anyio->httpx->llama-index-core==0.10.46->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.46->llama-index) (24.1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.46->llama-index)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.18.4 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.46->llama-index)\n",
            "  Downloading pydantic_core-2.18.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.46->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.10.46-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_core-0.10.46-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m411.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
            "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m787.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.1.22-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.1.25-py3-none-any.whl (37 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Using cached aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Using cached fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "Downloading llama_parse-0.4.4-py3-none-any.whl (8.0 kB)\n",
            "Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m441.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m330.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Downloading openai-1.34.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m194.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pillow-10.3.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m201.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached SQLAlchemy-2.0.30-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Using cached tiktoken-0.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m193.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
            "Using cached greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\n",
            "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m181.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
            "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m189.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.18.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m168.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Using cached regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m177.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Installing collected packages: striprtf, pytz, dirtyjson, wrapt, urllib3, tzdata, tqdm, tenacity, soupsieve, sniffio, regex, PyYAML, pypdf, pydantic-core, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, requests, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 async-timeout-4.0.3 attrs-23.2.0 beautifulsoup4-4.12.3 certifi-2024.6.2 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.6.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 joblib-1.4.2 llama-index-0.10.46 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.46 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.22 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.25 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.4 llamaindex-py-client-0.1.19 marshmallow-3.21.3 multidict-6.0.5 mypy-extensions-1.0.0 networkx-3.2.1 nltk-3.8.1 numpy-1.26.4 openai-1.34.0 pandas-2.2.2 pillow-10.3.0 pydantic-2.7.4 pydantic-core-2.18.4 pypdf-4.2.0 pytz-2024.1 regex-2024.5.15 requests-2.32.3 sniffio-1.3.1 soupsieve-2.5 striprtf-0.0.26 tenacity-8.3.0 tiktoken-0.7.0 tqdm-4.66.4 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.2 wrapt-1.16.0 yarl-1.9.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWJ3INxWUy6o"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NMJqmKJkUy6p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-7LZ8q_Uy6q",
        "outputId": "e3377aa7-82ed-47ee-a6a4-8eacf9f36be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-18 11:38:46--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "data/paul_graham/pa 100%[===================>]  73,28K   167KB/s    in 0,4s    \n",
            "\n",
            "2024-06-18 11:38:49 (167 KB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mghX2k7pUy6r"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DSk8WJ2ZUy6s"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data/laohac/\").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMAIPCbSUy6u"
      },
      "source": [
        "## Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "27497a47c7f1454187f45ba6b3e98702",
            "f7c68113bbce43ba9ba3328d9754e7b1",
            "8b8c4d722c8349648d112b4b3a41b071",
            "d429479688594c008219a32291e79d07",
            "c10e631412de4e20995ada102a29104e",
            "69de0bac19654339ad374f64219b170a",
            "10cf8bdcdb3f4547b0b800f25bd2f795",
            "e5bd5d8c0eb54afab36158bc8f69df91",
            "01239939f9b045f799e892b8a5e4b5e7",
            "ebacdb4969c04c7abba3282ec55e2211",
            "7a13ccb038d348409f7ecc666332a809"
          ]
        },
        "id": "ykdCSpmXUy6v",
        "outputId": "ffd8af63-9806-47c1-b176-1c73e0285995"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 48.69it/s]\n",
            "Extracting paths from text: 100%|██████████| 12/12 [00:09<00:00,  1.21it/s]\n",
            "Extracting implicit paths: 100%|██████████| 12/12 [00:00<00:00, 7986.62it/s]\n",
            "Generating embeddings: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n",
            "Generating embeddings: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import PropertyGraphIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "index = PropertyGraphIndex.from_documents(\n",
        "    documents,\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
        "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqj6RepQUy6x"
      },
      "source": [
        "So lets recap what exactly just happened\n",
        "- `PropertyGraphIndex.from_documents()` - we loaded documents into an index\n",
        "- `Parsing nodes` - the index parsed the documents into nodes\n",
        "- `Extracting paths from text` - the nodes were passed to an LLM, and the LLM was prompted to generate knowledge graph triples (i.e. paths)\n",
        "- `Extracting implicit paths` - each `node.relationships` property was used to infer implicit paths\n",
        "- `Generating embeddings` - embeddings were generated for each text node and graph node (hence this happens twice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sORiFgi9Uy6y"
      },
      "source": [
        "Lets explore what we created! For debugging purposes, the default `SimplePropertyGraphStore` includes a helper to save a `networkx` representation of the graph to an `html` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_pxicZpVc7A",
        "outputId": "7d0b8a5f-8dbb-42c9-cf9e-e3b497fc4e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from pyvis) (8.18.1)\n",
            "Collecting jinja2>=2.9.6 (from pyvis)\n",
            "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jsonpickle>=1.4.1 (from pyvis)\n",
            "  Downloading jsonpickle-3.2.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: networkx>=1.11 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from pyvis) (3.2.1)\n",
            "Requirement already satisfied: decorator in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (3.0.47)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (2.18.0)\n",
            "Requirement already satisfied: stack-data in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (5.14.3)\n",
            "Requirement already satisfied: typing-extensions in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (1.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.9.6->pyvis)\n",
            "  Using cached MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/nextai/miniconda3/envs/llama-index/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m161.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Downloading jsonpickle-3.2.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m287.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: MarkupSafe, jsonpickle, jinja2, pyvis\n",
            "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.4 jsonpickle-3.2.1 pyvis-0.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kPuPkxarUy6z"
      },
      "outputs": [],
      "source": [
        "index.property_graph_store.save_networkx_graph(name=\"./kg.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Qo5XRpUy60"
      },
      "source": [
        "Opening the html in a browser, we can see our graph!\n",
        "\n",
        "If you zoom in, each \"dense\" node with many connections is actually the source chunk, with extracted entities and relations branching off from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lg0oQTmUy61"
      },
      "source": [
        "![example graph](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/kg_screenshot.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYxL5ugAUy62"
      },
      "source": [
        "## Customizing Low-Level Construction\n",
        "\n",
        "If we wanted, we can do the same ingestion using the low-level API, leverage `kg_extractors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YsgANhjUy62"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.indices.property_graph import (\n",
        "    ImplicitPathExtractor,\n",
        "    SimpleLLMPathExtractor,\n",
        ")\n",
        "\n",
        "index = PropertyGraphIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
        "    kg_extractors=[\n",
        "        ImplicitPathExtractor(),\n",
        "        SimpleLLMPathExtractor(\n",
        "            llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
        "            num_workers=4,\n",
        "            max_paths_per_chunk=10,\n",
        "        ),\n",
        "    ],\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JkohvKSUy63"
      },
      "source": [
        "For a full guide on all extractors, see the [detailed usage page](../../module_guides/indexing/lpg_index_guide.md#construction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xetGz_a0Uy63"
      },
      "source": [
        "## Querying\n",
        "\n",
        "Querying a property graph index typically consists of using one or more sub-retrievers and combining results.\n",
        "\n",
        "Graph retrieval can be thought of\n",
        "- selecting node(s)\n",
        "- traversing from those nodes\n",
        "\n",
        "By default, two types of retrieval are used in unison\n",
        "- synoynm/keyword expansion - use the LLM to generate synonyms and keywords from the query\n",
        "- vector retrieval - use embeddings to find nodes in your graph\n",
        "\n",
        "Once nodes are found, you can either\n",
        "- return the paths adjacent to the selected nodes (i.e. triples)\n",
        "- return the paths + the original source text of the chunk (if available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg8IxpncUy64",
        "outputId": "8b2ec68c-00b0-4414-b201-e72f8991df10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interleaf -> Was -> On the way down\n",
            "Viaweb -> Had -> Code editor\n",
            "Interleaf -> Built -> Impressive technology\n",
            "Interleaf -> Added -> Scripting language\n",
            "Interleaf -> Made -> Scripting language\n",
            "Viaweb -> Suggested -> Take to hospital\n",
            "Interleaf -> Had done -> Something bold\n",
            "Viaweb -> Called -> After\n",
            "Interleaf -> Made -> Dialect of lisp\n",
            "Interleaf -> Got crushed by -> Moore's law\n",
            "Dan giffin -> Worked for -> Viaweb\n",
            "Interleaf -> Had -> Smart people\n",
            "Interleaf -> Had -> Few years to live\n",
            "Interleaf -> Made -> Software\n",
            "Interleaf -> Made -> Software for creating documents\n",
            "Paul graham -> Started -> Viaweb\n",
            "Scripting language -> Was -> Dialect of lisp\n",
            "Scripting language -> Is -> Dialect of lisp\n",
            "Software -> Will be affected by -> Rapid change\n",
            "Code editor -> Was -> In viaweb\n",
            "Software -> Worked via -> Web\n",
            "Programs -> Typed on -> Punch cards\n",
            "Computers -> Skipped -> Step\n",
            "Idea -> Was clear from -> Experience\n",
            "Apartment -> Wasn't -> Rent-controlled\n"
          ]
        }
      ],
      "source": [
        "retriever = index.as_retriever(\n",
        "    include_text=False,  # include source text, default True\n",
        ")\n",
        "\n",
        "nodes = retriever.retrieve(\"What happened at Interleaf and Viaweb?\")\n",
        "\n",
        "for node in nodes:\n",
        "    print(node.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVUZTJfpUy64",
        "outputId": "7a474980-080b-4aec-bb71-ad385ce20929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interleaf had smart people and built impressive technology, including adding a scripting language that was a dialect of Lisp. However, despite their efforts, they were eventually impacted by Moore's Law and faced challenges. Viaweb, on the other hand, was started by Paul Graham and had a code editor where users could define their own page styles using Lisp expressions. Viaweb also suggested taking someone to the hospital and called something \"After.\"\n"
          ]
        }
      ],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    include_text=True,\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"What happened at Interleaf and Viaweb?\")\n",
        "\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxKE2DjhUy65"
      },
      "source": [
        "For full details on customizing retrieval and querying, see [the docs page](../../module_guides/indexing/lpg_index_guide.md#retrieval-and-querying)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgr5ZWuMUy65"
      },
      "source": [
        "## Storage\n",
        "\n",
        "By default, storage happens using our simple in-memory abstractions - `SimpleVectorStore` for embeddings and `SimplePropertyGraphStore` for the property graph.\n",
        "\n",
        "We can save and load these to/from disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmzfwNmaUy65"
      },
      "outputs": [],
      "source": [
        "index.storage_context.persist(persist_dir=\"./storage\")\n",
        "\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "index = load_index_from_storage(\n",
        "    StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXhj2OiSUy66"
      },
      "source": [
        "### Vector Stores\n",
        "\n",
        "While some graph databases support vectors (like Neo4j), you can still specify the vector store to use on top of your graph for cases where its not supported, or cases where you want to override.\n",
        "\n",
        "Below we will combine `ChromaVectorStore` with the default `SimplePropertyGraphStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXc9b9aIUy66"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-ETyoXMUy66"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.graph_stores import SimplePropertyGraphStore\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "\n",
        "client = chromadb.PersistentClient(\"./chroma_db\")\n",
        "collection = client.get_or_create_collection(\"my_graph_vector_db\")\n",
        "\n",
        "index = PropertyGraphIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
        "    graph_store=SimplePropertyGraphStore(),\n",
        "    vector_store=ChromaVectorStore(collection=collection),\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "index.storage_context.persist(persist_dir=\"./storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daCO75K3Uy67"
      },
      "source": [
        "Then to load:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5FVVxe8Uy67"
      },
      "outputs": [],
      "source": [
        "index = PropertyGraphIndex.from_existing(\n",
        "    SimplePropertyGraphStore.from_persist_dir(\"./storage\"),\n",
        "    vector_store=ChromaVectorStore(chroma_collection=collection),\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HUU-WmcUy67"
      },
      "source": [
        "This looks slightly different than purely using the storage context, but the syntax is more concise now that we've started to mix things together."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llama-index-bXUwlEfH-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01239939f9b045f799e892b8a5e4b5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10cf8bdcdb3f4547b0b800f25bd2f795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27497a47c7f1454187f45ba6b3e98702": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7c68113bbce43ba9ba3328d9754e7b1",
              "IPY_MODEL_8b8c4d722c8349648d112b4b3a41b071",
              "IPY_MODEL_d429479688594c008219a32291e79d07"
            ],
            "layout": "IPY_MODEL_c10e631412de4e20995ada102a29104e"
          }
        },
        "69de0bac19654339ad374f64219b170a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a13ccb038d348409f7ecc666332a809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b8c4d722c8349648d112b4b3a41b071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5bd5d8c0eb54afab36158bc8f69df91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01239939f9b045f799e892b8a5e4b5e7",
            "value": 1
          }
        },
        "c10e631412de4e20995ada102a29104e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d429479688594c008219a32291e79d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebacdb4969c04c7abba3282ec55e2211",
            "placeholder": "​",
            "style": "IPY_MODEL_7a13ccb038d348409f7ecc666332a809",
            "value": " 1/1 [00:00&lt;00:00,  6.63it/s]"
          }
        },
        "e5bd5d8c0eb54afab36158bc8f69df91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebacdb4969c04c7abba3282ec55e2211": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c68113bbce43ba9ba3328d9754e7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69de0bac19654339ad374f64219b170a",
            "placeholder": "​",
            "style": "IPY_MODEL_10cf8bdcdb3f4547b0b800f25bd2f795",
            "value": "Parsing nodes: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
